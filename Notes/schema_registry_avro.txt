Chapter 2 : AVRO Schemas
------------------------

What is Avro
------------
AVRO - has a schema and a payload

Advantages of using AVRO
    - Data is fully typed
    - Data is compressed automatically
    - Schema (defined using JSON) also has data
    - Documentation is embedded in the schema
    - Data can be read across any language
    - Schema can evolve over time in a safe manner

Disadvantage
    - All programming languages may not support AVRO
    - Since data is compressed and serialized , its not possible to print the data without using specialized tools.

Primitive types
----------------
null : no value
boolean : a binary value
int : 32 bit signed integer
long : 64 bit signed integer
float : single precision 32 bit IEEE 754 floating-point number
double : double precision 64 bit IEEE 754 floating-point number
bytes : sequence of 8 bit unsigned bytes
string : unicode character sequence

Avro record schema
------------------
Avro record schemas are defined in JSON
It has some common fields
Name : Name of your schema
Namespace : equivalent of package in java
Doc : documenation for the Schema
Aliases : Optional Other names for your schema
Fields : Array of fields on the record
    Each field will contain
    Name : name of the field
    Doc : documenation for the field
    Type : data type for that field (can be primitive)
    Default : default value for the field

Excercise - Defining an Avro schema for customer
Customer has
    - First Name
    - Last Name
    - Age
    - Height (in cms)
    - Weight (in kgs)
    - Automated email turned on (Boolean , default true)

Complex Types
-------------
 - Enums
 - Arrays
 - Maps
 - Unions
 - Calling other schemas as types

 Enums - Note - Once an Enum is set, changing the enum values is forbidden if you want to main compatibility
 Arrays - List of undefined size for items that share the same schema
 Maps - List of keys and values, where keys are strings, values can be of different types
 Unions - can allow a field to take different values. Ex - ["string", "int", "boolean"]. Field can be an int or a string or a boolean.
    If defaults are defined for Unions then default should be of type of first Union item. Referring to the above example, if default is
    defined then it should be of value "string". Most common use case for Unions is to define an Optional Value. -
    {"name": "middle_name", "type": ["null", "string"], "default": null].
    PS - "null" is not same as null

Avro Schema Practice 2
----------------------
Customer has
    - First Name
    - Middle Name (Optional)
    - Last Name
    - Age
    - Height (in cms)
    - Weight (in kgs)
    - Automated email (Boolean , default true)
    - Customer emails (array), default is an empty array
    - customer Address (This is a record in itself)
        - Address
        - City
        - Postcode (String or number)
        - Type (Enum with values - PO BOX, RESIDENTIAL, ENTERPRISE)

Customer emails field has a default value of empty array

Logical Types
-------------
Logical types are added to give more meaning to primitive types.
Most commonly used

decimal - bytes
date - int - number of days since unix epoch (Jan 1st 1970)
time-millis - long - number of milliseconds after midnight, 00:00:00.000
timestamp-millis - long - number of milliseconds since unix epoch (epoch - 1 January 1970 00:00:00.000 UTC)

customer_signup_ts field on the customer-complex.avsc

Complex case of Decimals
------------------------
Floats and doubles are called floating point binary types. In memory they represent something like -  10001.10010110011
They are just binary types.

Decimal is  floating decimal point type. They represent a number like - 12345.6789
PS - some decimals cannot be represented accurately as floats or doubles

Floats and Doubles are good for scientific calculation and such that dont need a high level of accuracy,
decimal  is used for representing money fields. For money fields we dont want to be off even by tiny bit, high precision is needed.

In Avro there is decimal LogicalType, but its underlying type is bytes, which means you cant really print decimal as JSon
Since their underlying representation is bytes, it will be printed as gibberish.

how to deal with this issue ? Teacher's recommendation is to use String as parsing of String into decimals is easy

Chapter 3 : Avro in Java
-------------------------

Generic Record in Avro
----------------------
A Generic Object is used to create a Java object from Avro schema, schema being referenced as
 - File
 - String

This approach of using the Generic Record is not recommended as things can fail at runtime but its the most simple way to do so

Generic Record is not Type safe, so it might result in failures during runtime.

Specific Record in Avro
-----------------------
Specific Record is also an Avro Object, it is obtained from code generation of avro schemas.

Avro tools - hands on
---------------------
a jar is available to read contents from an avro file generated by the java program with data, ex -  customer-generic-generated.avro
and customer-specific-generated.avro

Commands for avro-tools-1.8.2.jar
---------------------------------
to list all the commands available for avro-tools-1.8.2.jar :: java -jar avro-tools-1.8.2.jar
to read customer-generic-generated.avro :: java -jar avro-tools-1.8.2.jar tojson --pretty customer-generic-generated.avro
to read customer-specific-generated.avro :: java -jar avro-tools-1.8.2.jar tojson --pretty customer-specific-generated.avro
to get schema of any file :: java -jar avro-tools-1.8.2.jar getschema customer-specific-generated.avro

kafka-avro-consumer uses this jar implicitly

Reflection in  Avro
-------------------
Reflection in  avro can be used to build schema from an existing class


java -jar avro-tools-1.8.2.jar tojson --pretty customer-reflection-generated.avro
java -jar avro-tools-1.8.2.jar getschema customer-reflection-generated.avro

Schema Evolution
----------------
Data is in the form of contract between Producer and Consumer

Avro enables us to evolve the schema over time , Ex - today in version 1.0 we might accept only first_name and last_name of a customer,
in future phone_number might be added, then it becomes ver 2.0
version 1.0 :: first_name, last_name
version 2.0 :: first_name, last_name, phone_number

There are 4 types of Schema evolution
1) Backward : backward compatible change is when a new schema can be used to read old data
Ex - v2 Schema can be used to read v1 data

We can provide backward compatibility by creating the new fields on the new version with defaults

2) Forward : forward compatible change is when an old schema can be used to read new data
Ex - v1 schema can be used to read v2 data

Avro will ignore the new fields. Deleting fields without defaults is not forward compatible

We want forward compatible when we want to make a data stream evolve without changing our downstream consumers.

3) Full : Combination of both Forward and Backward

Only add fields with defaults
Only remove fields that have defaults
Always target for "full" compatibility when making schema changes

4) Breaking : none of the above

Breaking changes are introduced when -
    Adding or removing elements from enum is non-compatible change.
    Changing the type of a field , for ex - from String to int
    Renaming a required field (a required field is one that has no default value)

Do not do non-breaking changes

6 Strongly recommended points while writing an Avro Schema

    1) Make your primary key required
    2) Provide defaults to all those fields that could be removed in the future
    3) Be very careful with  enums as enums cannot evolve over time
    4) Do not rename fields. You can add aliases instead
    5) when evolving a schema , always provide defaults
    6) when evolving a schema, never delete a required field

Schema Evolution - Hands On
---------------------------
This hands on is an example of Full compatible change.

2 new fields with defaults are added to Customer V2 - Phone_number and email
1 field from version 1 is removed - automated_email, this field has default

Chapter 4 - Setup and Launch kafka
----------------------------------
Landoop is the image that comes packaged with UI for schema registry, kafka broker, kafka connect , landoop UI and kafka topics
From inside the kafka-avro directory run :: "docker-compose up" to bring up landoop UI
http://localhost:3030 :: landoop UI there by UI for schema-registry, topics, kafka connect and kafka brokers

Download confluent tools in one of 2 ways
1) by downloading confluent tools, this approach didnt work for me
2) running the commands
     docker pull confluentinc/cp-schema-registry
     docker run -it --rm --net=host confluentinc/cp-schema-registry bash

Once inside the container execute the command
    - kafka-avro-console-consumer


Chapter 5 - Confluent Schema Registry and Kafka
------------------------------------------------

Confluent Schema Registry
-------------------------
Schema registry is to store and retrieve schemas for Consumer and Producers
Enforce Backward / Forward / Full Compatibility on Topics
Decrease the size of data payload sent to Kafka

Operations - Refer to - https://docs.confluent.io/platform/current/schema-registry/develop/api.html  for REST operations
----------
We add Schemas
Retrieve a Schema
Update a Schema
Delete a Schema
All these operations through REST API

Schemas can be applied to Keys and or values

to access landoop UI - Run docker-compose up and then access the UI :: http://localhost:3030

I created the topic - kafka-topics --bootstrap-server localhost:9092 --topic customer-topic --create --partitions 3 --replication-factor 1

Avro Console Producer and Console Consumer
------------------------------------------
I created the topic as :: kafka-topics --bootstrap-server localhost:9092 --topic customer-topic --partitions 3 --replication-factor 1 --create

I ran confluent tools image :: docker run -it --rm --net=host confluentinc/cp-schema-registry bash

kafka-avro-console-producer
---------------------------
From inside the container I ran ::  kafka-avro-console-producer --bootstrap-server localhost:9092 --topic customer-topic --property schema.registry.url=http://localhost:8081 --property value.schema.id=22
Then added new records -

{"first_name": "John","last_name": "Doe","age": 29,"height": 167.77}
{"first_name": "John2","last_name": "Doe2","age": 45,"height": 180}
{"first_name": "John3","last_name": "Doe3","age": 51,"height": 178}
{"first_name": "John4","last_name": "Doe4","age": 83,"height": 199}
{"first_name": "John5","last_name": "Doe5","age": 83,"height": 199}
{"first_name": "John6","last_name": "Doe6","age": 83,"height": 199}
{"first_name": "John7","last_name": "Doe7","age": 83,"height": 199}
{"first_name": "John8","last_name": "Doe8","age": 83,"height": 199}

{"first_name": "John9","last_name": "Doe9","age": 83,"height": 199}
{"first_name": "John10","last_name": "Doe10","age": 83,"height": 199}

{"first_name": "John11","last_name": "Doe10","age": 83,"height": 199, "weight": 100}
{"first_name": "John12","last_name": "Doe12","age": 83,"height": 199, "weight": 67}

kafka-avro-console-consumer
----------------------------
From inside the container I ran ::  kafka-avro-console-producer --bootstrap-server localhost:9092 --topic customer-topic --property schema.registry.url=http://localhost:8081 --property value.schema.id=22

kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic customer-topic --partition 2 --property schema.registry.url=http://localhost:8081
If no offset is specified this consumer reads from the end of the topic/partition (messages posted after the consumer started)

kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic customer-topic --partition 2 --offset 1 --property schema.registry.url=http://localhost:8081
This consumer read from the offset 1 (including message with offset 1)


kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic customer-topic --partition 2 --from-beginning --property schema.registry.url=http://localhost:8081
Read all the messages on the partition

kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic customer-topic  --from-beginning --property schema.registry.url=http://localhost:8081
If no Partition is specified then this consumer will read from all the partitions

A schema cannot be evolved unless its compatible with the previous version

Kafka Avro Producer in Java
----------------------------
All details are in the class KafkaAvroProdicerV1.java, This class only produces the message,
for consuming the message I used console consumer as -
    kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic customer-topic --from-beginning --property schema.registry.url=http://localhost:8081

If Topic does not exist then it gets auto created by KafkaProducer based on the default settings in kafka/BootStrap Server

Kafka AVRO Consumer in Java
---------------------------



















