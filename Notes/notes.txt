Chapter - Kafka Theory
---------------------

Chapter - Starting Kafka
------------------------
brew install kafka

brew services start zookeeper

brew services start kafka

To have launchd start zookeeper now and restart at login:
Â  brew services start zookeeper
Or, if you don't want/need a background service you can just run:
Â  zkServer start
==> Summary
ðŸºÂ  /usr/local/Cellar/zookeeper/3.7.0: 1,073 files, 42.4MB
==> Installing kafka
==> Pouring kafka--2.8.0.catalina.bottle.tar.gz
==> Caveats

To have launchd start kafka now and restart at login:
Â  brew services start kafka
Or, if you don't want/need a background service you can just run:
Â  zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties & kafka-server-start /usr/local/etc/kafka/server.properties
==> Summary
ðŸºÂ  /usr/local/Cellar/kafka/2.8.0: 200 files, 68.2MB
==> Caveats

==> zookeeper
To have launchd start zookeeper now and restart at login:
Â  brew services start zookeeper
Or, if you don't want/need a background service you can just run:
Â  zkServer start
==> kafka
To have launchd start kafka now and restart at login:
Â  brew services start kafka
Or, if you don't want/need a background service you can just run:
Â  zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties & kafka-server-start /usr/local/etc/kafka/server.properties


ZooKeeper
---------
As daemon process -
	zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties
Not as daemon process -
	zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties

Kafka Server
------------
Not as daemon process -
	kafka-server-start /usr/local/etc/kafka/server.propertiesÂ 
As daemon process -
	kafka-server-start -daemon /usr/local/etc/kafka/server.properties

Start both zookeeper and kafka together
----------------------------------------
zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties & kafka-server-start /usr/local/etc/kafka/server.properties

Another way of starting kafka is - docker container - docker run confluentinc/cp-enterprise-kafka:5.2.2
This didnt work for me as it was asking for zookeeper

Chapter - Command Line Interface 101
------------------------------------

Kafka Topic Commands
-------------------------------
Creating a topic -
	With specific configuration
		kafka-topics --bootstrap-server localhost:9092 --topic first-topic --create --partitions 3 --replication-factor 1
	With all defaults -
		kafka-topics --bootstrap-server localhost:9092 --topic first-topic --create
Listing topics -
	 kafka-topics --bootstrap-server localhost:9092 --list
Description of a topic -
	kafka-topics --bootstrap-server localhost:9092 --topic first-topic --describe
Deleting a topic -
	kafka-topics --bootstrap-server localhost:9092 --topic first-topic --delete

Kafka-Console-Producer Commands
------------------------------------------------
kafka-console-producer --bootstrap-server localhost:9092 --topic first-topic
There will be a little caret , start typing ur messages. If there is no error messages are successfully sent.
Use Ctl+c to exit the console producer

Producing messages to a non-existent topic ::
	kafka-console-producer --bootstrap-server localhost:9092 --topic non-existing-topic
I changed the default # of partitions by editing the server.properties file at /usr/local/etc/kafka and restarted kafka admin client

After that ::
	kafka-console-producer --bootstrap-server localhost:9092 --topic non-existing-topic_2
New non-existing topic non-existing-topic_2 was created with 3 partitions

Teacher's recommendation is to manually create a topic and then produce messages.

Kafka-Console-Consumer Commands
--------------------------------------------------
Basic command for consuming -
	kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic
But with this command Messages sent prior to starting consumer will not be consumed. Only those Messages sent after the consumer started will be consumed.

To read the messages from the beginning of the topic use the command -
	kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic --from-beginning


Kafka-Console-Consumers in a Group
--------------------------------------------------
Command to start the consumers in a group -
	kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic --group second-consumer-group
Command to start the consumers in a new group to read from start of the topic from beginning
	kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic --group second-consumer-group --from-beginning
Command to start a consumer in a group with certs
    kafka-console-consumer --bootstrap-server <broker-name>:9093 --topic cc-serverdb-tap2aws --consumer.config client-ssl.properties --group ccordersv3-local-group --from-beginning


Kafka remembers the offset(index) on every topic that consumer(s) of a group consumed. If u start a consumer of an existing group on existing topic with --from-beginning command it only reads those messages that are not already consumed by an active or a non-active consumer of the same group.
If u start new consumer on new group for a topic that has a bunch of messages on each partition (offset of each partition is > zero) then by default LAG for the new consumer group is zero, meaning new group will not read old messages, will read only new messages, if u want this group to read all the messages from the beginning , then use  --from-beginning option

To see how this works please spin up multiple consumers in the same group


Kafka-consumer-Groups
---------------------------------

List all the consumer groups -
	kafka-consumer-groups --bootstrap-server localhost:9092 --list

Describe a consumer group -
	kafka-consumer-groups --bootstrap-server localhost:9092 --group first-consumer-group --describe
	Lag indicates the delta between CURRENT-OFFSET and LOG-END-OFFSET

Start a consumer of group first-consumer-group in a different Terminal and the run this command to see what output we get
	kafka-consumer-groups --bootstrap-server localhost:9092 --group first-consumer-group --describe
That will list the active consumers and which partitions they are listening to , etc

Reset offset
-----------------
Reset offset will only reset the offset of a specific consumer group
	kafka-consumer-groups --bootstrap-server localhost:9092 --topic first-topic --group first-consumer-group --reset-offsets --to-earliest --execute
Above command resets the offset of first-consumer-group to earliest (0 in this case) , this option is very different from --from-beginning.

	kafka-consumer-groups --bootstrap-server localhost:9092 --group second-consumer-group --topic first-topic --reset-offsetsÂ  --shift-by -2Â  --execute
Above command will reset the offset only by few indexes so the consumer group can read only those and not all from beginning

Resetting offset based on a particular offset
    - kafka-consumer-groups --bootstrap-server <broker-name>:9093 --group ccorders-v3-prod --command-config client-prod-ssl.properties --topic fit-sparta-retail-transactions:0 --reset-offsets  --to-offset 319814955  --execute
      kafka-consumer-groups.sh --bootstrap-server kafka-ttc-app.dev.target.com:9092 --topic item-changes:0 --group ted-test-group-id --reset-offsets --to-offset 14789764 --execute
Resetting offset for a specific Partition
    - kafka-consumer-groups --bootstrap-server localhost:9092 --group tweets-consumer-group --topic twitter-tweets:2 --reset-offsets --to-offset 15 --execute

Decode and encoded file :: base64 -d -i <input file name> -o <output file name> OR base64 -d copy_and_paste_the_string

Another way to run the cli is to run inside the docker container
    docker run --net=host --name=kafka-consumer -v ~/Documents/Rekha/workspace/kafka_keys/:/kafka_keys -it --rm confluentinc/cp-kafka:7.3.0 /bin/bash
    Run any kafka cli command of you choice

Chapter - Kafka Java Programming 101
------------------------------------

Kafka producer can follow one of the 3 patterns for producing the message to kafka topic
1) Fire and forget
Ex - BasicProducer
kafkaProducer.send(producerRecord);

2) Synchronoud producer
ex - BasicProducerWithKey
  RecordMetadata = kafkaProducerWithKey.send(producerRecord, new MyProducerCallback())
                    .get();

3) Asynchronous producer - using callback
ex - BasicProducerWithCallback
kafkaProducer.send(producerRecord, new MyProducerCallback());

Kafka Producer with Callback
----------------------------
public void onCompletion(RecordMetadata metadata, Exception exception) {
// this executes every time a message is successfully sent or an exception is thrown

Kafka Producer with Key
-----------------------
Messages with same key goes the same partition
by default producer.send is asynchronous, it can be synchronous by adding get() method. Look at the BasicProducerWithKey.
Run this sample multiple time to see that same key goes to same partition.

Message Key : id_0
Partition : 1

Message Key : id_1
Partition : 0

Message Key : id_2
Partition : 2

Message Key : id_3
Partition : 0

Message Key : id_4
Partition : 2

Message Key : id_5
Partition : 2

Message Key : id_6
Partition : 0

Message Key : id_7
Partition : 2

Message Key : id_8
Partition : 1

Message Key : id_9
Partition : 2

Kafka Consumer
---------------
For AUTO_OFFSET_RESET_CONFIG , there are 3 values available
    earliest - from the beginning of the topic
    latest - from the point consumer is added to topic onwards
    none - will throw error

One consumer can consume from more than one topic

Java Consumer inside a consumer group
-------------------------------------
you can run the Consumer Java application more than once to create multiple consumers in a group.
When u spin and stop consumers look for messages in the console to check for how many stopped and how re-assignment happens.

Ex - message from BasicConsumer1 after 2 and 3 were started
09:19:30.513 [main] INFO  o.a.k.c.c.i.AbstractCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Attempt to heartbeat failed since group is rebalancing
09:19:30.515 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Revoke previously assigned partitions first-topic-1, first-topic-0
09:19:30.515 [main] INFO  o.a.k.c.c.i.AbstractCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] (Re-)joining group
09:19:30.516 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Finished assignment for group at generation 22: {consumer-first-java-consumer-group-1-6cfecd2a-68ce-47e5-b990-9040dac7edf4=Assignment(partitions=[first-topic-0]), consumer-first-java-consumer-group-1-ffcf31b6-ee27-4e13-96aa-3398669698a0=Assignment(partitions=[first-topic-2]), consumer-first-java-consumer-group-1-948df6b3-b41b-4ede-b7f0-191f95971117=Assignment(partitions=[first-topic-1])}
09:19:30.518 [main] INFO  o.a.k.c.c.i.AbstractCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Successfully joined group with generation 22
09:19:30.518 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Notifying assignor about the new Assignment(partitions=[first-topic-0])
09:19:30.518 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Adding newly assigned partitions: first-topic-0
09:19:30.519 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Setting offset for partition first-topic-0 to the committed offset FetchPosition{offset=57, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.16:9092 (id: 0 rack: null)], epoch=0}}


Java Consumer with Threads
--------------------------
Refer to ConsumerAsThread.java for example, Its slightly complicated.

Java Consumer Assign and Seek
------------------------------
Assign and Seek are mostly used to replay data and fetch a specific message

Client Bi-Directional Compatibiity
----------------------------------
API calls are now versioned. This means
    older clients (ex - 1.1) can talk to newer brokers (2.0 +)
    newer cluents (2.0 +) can talk to older brokers (1.1)

As far as possible - use latest client irrespective of broker versions

Chapter - Kafka Twitter Producer and Advanced Configurations
-------------------------------------------------------------

Twitter setup
-------------
userName - rekha_bg@yahoo.com
API Key - Your key is like your username. It is used to verify who you are to Twitter.
	- F7JikmUt6SkQHYiZXFbgVpTZ4
API Secret Key - Your secret key is used like a password. It identifies your account. Keep this safe!
	- 4yKV8qO1ttlYX3E31BWTYqJpXYDoLQv3FBI7lpvYOTqp97cRUG
Bearer Token - An Access Token used in authentication that allows you to pull specific data.
	- AAAAAAAAAAAAAAAAAAAAAMJlRAEAAAAA%2BglBMiKdMeG%2BJTHNdZxAtZ5faVQ%3DEzKy1qfcmHA2OapS0CWFkromTEme3XtyP5unFdqBZ4oHueIsly
Access Token
        - 2746149770-u6SdzYuuehtoIVKNUQYg6mMLQ5ybmnXHuOnOQst
Access Token Secret
        - 6BQP5FYgYAchSITWuTYcvBKYjDc1Pa4apXHnOwgYODZNc


An access token and access token secret are user-specific credentials used to authenticate OAuth 1.0a API requests. They specify the Twitter account the request is made on behalf of.



kafka-topics --bootstrap-server localhost:9092 --topic twitter-tweets --create --partitions 3 --replication-factor 1

After starting TwitterProducer class , run the console consumer with the command -
    kafka-console-consumer --bootstrap-server localhost:9092 --topic twitter-tweets

        //flush data.
        //Flush actually causes queued records to be sent and blocks until they are completed (successfully or otherwise).
        // Calling get() immediately doesn't trigger anything to happen
        kafkaProducer.flush();

Acks and Min.Insync.replicas
----------------------------

3 levels for acks - 0, 1 and all

acks = 0 - No response is requested from the broker. If broker goes offline or an exception occurs, producer wont know and data will be lost.
This setting is useful when its okay to loose the data. Producer will not know if the message made it to the broker or not.
acks = 0 is good for performance, Good use cases are - metrics, Log collection, etc

acks = 1 - Default settings as of Kafka 2.0 is acks = 1. This means leader response is requested but replication is not guaranteed meaning in-sync replicas confirmation of message receival is not processed.
Usually happens in the backgroud.
If ack is not received then producer may re-try.
Producer knows the broker has the data.
If leader goes down before replication happens the data will be lost.

acks = all - Leader and replicas both are requested to acknowledge. Broker after receiving the acknowledgement from the leader and all replicas
sends the ack back to producer. This setting is the safest and adds latency (not much). This is a necessary setting for safe Producer.
If there are enough replica sets with this setting there is no data loss.
With acks = all , there is another setting - min.insync.replicas

min.insync.replicas , is broker or topic setting. broker setting can be overwritten with topic setting.
min.insync.replicas = 2 implies there are atleast 2 brokers that are ISR (including the leader) that must respond with the ack.
min.insync.replicas = 2 is the common setting

If replication factor = 3, min.insync.replicas = 2 and acks = all
    - producer can only tolerate one broker going down otherwise producer will receive an exception on send (NOT_ENOUGH_REPLICAS)

For maximum safety, maximum availability - replication factory = 3, min.insync.replicas 2, and acks = all

Retries and Max.in.flight.requests.per.connection
--------------------------------------------------

There are multiple properties related to retry logic,

developers are expected to handle transient failure like NotEnoughReplicasException, if not handled and data not re-sent,
data will be lost.

For kafka >= 2.1 default retry count is a large number - 2147483647, this means producer re-tries 2147483647 times
Other default settings related to retries are -
delivery.timeout.ms = 120000 ms = 2 mins. Records will fail if they cannot be acknowledged within delivery.timeout.ms
max.in.flight.requests.per.connection - how many producer requests can be made in parallel
When re-try comes into play, messages can get out of order.

No need to worry about this as my version of kafka is greater than 2.1


Idempotent Producers
--------------------

Kafka >=2.1 has a very high number of retries - 2147483647 by default.
Lets consider a scenario to understand this use case -
1) Producer successfully sent message to Broker, broker received the message and committed but ack sent by broker didnt reach the Producer.
2) Producer since didnt receive the message sent the message again, broker received a duplicate message and sent the ack back
that reached the producer.
3) Per producer there was only 1 successful message but for broker there are duplicates.

Idempotent Producer sends a produce_request_id which is used by the broker to identify duplicate requests, broker commits the message only once

Idempotent Producers are great to guarantee a stable and safe pipeline
by default these are these settings of Producers if using kafka version >= 2.1. These settings apply automatically after producer starts if
u dont overwrite them manually.

retries = Integer.MAX_VALUE (2147483647)
max.in.flight.requests.per.connection = 5 (Kafka >= 1.0) - this is the number of requests per connection. Kafka maintains ordering even if there are retries.
acks = all

Teacher recommends to set these settings manually so its explicit in the code and obvious whats happening.

How do we set idempotent porducers -
    producerProps.put ("enable.idempotence", true)

Safe Producers
---------------

summarizing the settings here -
 - min.insync.replicas = 2 - at the topic or broker level, topic leve settings overwrite the broker level settings

at the Producer level
enable.idempotence =  true
acks = all,
retries = MAX_INT (by default)
max.in.flight.requests.per.connection = 5 (default)

while keeping the ordering guaranteed and improved performance.

PS - Running a safe producer might impact the throughput and latency but its not big

Refer to the TwitterProducer for setting these properties.

Producer Compression
--------------------

Compression is enabled at the Producer level, it does not require any configuration changes either in broker or in consumer.
"compression.type" can be 'none' (default), 'gzip', 'Iz4', 'snappy' and 'Zstd'
Compression is more effective when the batch of kafka messages is bigger, more u send to Kafka , compression is more helpful.

Advantages of compression
- much smaller producer request message (compression ratio can go upto 4x)
- Faster to transfer data over the network as message size is smaller
- less latency
- Better throughput
- Better disk Utilization in kafka (Stores messages on disk are smaller)

Disadvantage
- Producer uses some CPU cycles to compression
- Consumers also should use CPU cycles to uncompress the data

gzip has highest compression ratio but its not fast

From an article - https://portavita.github.io/2021-01-25-Why_Kafka_compression_might_save_you_thousands_of_dollars/#:~:text=Consumer%20reads%20data%20and%20decompresses,used%20to%20compress%20and%20decompress.
Kafka compression is about (ehm..) compressing data instead of sending data as they are.
Some data responds better to compression than other and some algorithm performs better than others.
Possible compression options in the producer are: none, gzip, snappy, lz4 and zstd.
Compression can be handled by the producer. That is: producer compresses data before sending to Kafka.
Consumer reads data and decompresses it. Kafka will store data in a compressed form.
That saves: bandwidth, disk space, RAM at the price of some CPU used to compress and decompress.
The outcome of our research might be different from your case.


Producer batching - this is useful in performance improvement
-------------------------------------------------------------

Kafka tries to send the messages as soon as possible to reduce the latency.

max.in.flight.requests.per.connection = 5, upto 5 messages produced individually can be sent at the same time.
If there are more messages ready to be sent while 5 messages are in flight, then kafka starts batching the messages so it can send them all in one go.

Smart batching allows kafka to have high throughput while maintaining low latency.
Batches have higher compression ratio so better efficiency. Each batch is considered one request.

There are 2 settings for batching
- linger.ms and batch.size

linger.ms - Number of millisecs a producer is willing to wait before sending out a batch (default 0)
By introducing a small delay linger.ms = 5, we increase the chances of messages being sent as batch, this batching helps with
higher throughput, efficiency and compression of producer

If batch is full (batch.size) before the linger.ms ends, then producer sends the message to kafka.

batch.size - Maximum number of bytes that will be included in a batch. by default it is 16KB
Any message bigger than the batch.size will not be batched.
batch is allocated per partition

High throughput producer
------------------------










Chapter - Kafka Elasticsearch Consumer and Advanced Configurations
------------------------------------------------------------------

Setting up Elasticsearch in the cloud
-------------------------------------
I setup elasticsearch in the cloud through bonsai.io. user_name is rekha_bg@yahoo.com and password is
cluster name - KAFKA_STANDALONE_cluster.
URL to access the cluster - https://app.bonsai.io/clusters/kafka-standalone-cou-1278981902/

Elasticsearch 101
-----------------
in the console (click on the Menu item "console" on the left Menu). I executed a REST api to get all the information about the cluster
GET /_cat/nodes?v

to get the list of all indices
GET /_cat/indices?v

Creating an index -
PUT /twitter
Response I got
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "twitter"
}
Then run GET /_cat/indices?v again. Will show details of twitter index

response I got
health status index                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   twitter L19J7L2zQUCa91eRIM620g   1   1          0            0       416b           208b

Inserting data into the Index (/twitter) and type (/twitter/tweets) -
PUT /twitter/tweets/1
Body
{
   "course": "Kafka for Beginners",
   "Instructor": "Stephane M",
   "module": "ElasticSearch"
}

Response I received
{
  "_index": "twitter",
  "_type": "tweets",
  "_id": "1",
  "_version": 1,
  "result": "created",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 0,
  "_primary_term": 1
}

If u try to PUT again it updates the same record. version in the response will be 2 this time.

if u want to get the data run :: GET /twitter/tweets/1

Response I got
{
  "_index": "twitter",
  "_type": "tweets",
  "_id": "1",
  "_version": 2,
  "_seq_no": 1,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "course": "Kafka for Beginners",
    "Instructor": "Stephane M",
    "module": "ElasticSearch"
  }
}

if u want to delete the record :: DELETE /twitter/tweets/1

If u want to delete the index :: DELETE /twitter
Response
{
  "acknowledged": true
}

Consumer Part - 1
-----------------
Calling the ElasticSearchConsumer class without creating the index "twitter" will fail. Please make sure index "twitter" is available.
U can do that by going to elastic search console and running
GET /twitter
Response I received
{
  "twitter": {
    "aliases": {},
    "mappings": {
      "properties": {
        "Instructor": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "course": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "module": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        }
      }
    },
    "settings": {
      "index": {
        "number_of_shards": "1",
        "auto_expand_replicas": null,
        "provided_name": "twitter",
        "creation_date": "1635781357068",
        "priority": "0",
        "number_of_replicas": "1",
        "uuid": "LozzR9xtQuqhVFE5BTxlWQ",
        "version": {
          "created": "7100299"
        }
      }
    }
  }
}


Consumer Part 2
----------------
Consume the message from twitter topic and send to Elastic search

Delivery Semantics for Consumers
--------------------------------
There are 3 modes of commiting the offsets
1) At Most Once

    Offsets are commited as soon as message batch is received. If processing goes wrong then messages will not be read again.
    there by causing message loss.

    Ex - Lets say 5 messages are read in batch, consumer committed the offset as soon as batch was read so offset is at 5
    processing of 3 went fine and then consumer crashed leaving 2 unprocessed messages.
    In this case When consumer group comes back up it will start reading the messages from offset 5 so 2 previous messages are skipped.

    There is a big chance of loosing data.

2) At Least Once

    Is default behavior. Offsets are committed only after message processing is successful. This might result in duplicate messages.
    Consumer should implement Idempotent processing of data.

    Ex - Lets say 5 messages are read in batch, consumer starts processing the messages.
         Processing of 3 went fine and then consumer crashed leaving 2 unprocessed messages. Offset is not committed yet.
         In this case When consumer group comes back up it will start reading the messages from offset 0 resulting in 3 duplicate messages.
         Once all 5 messages are successfully processed Offset is committed at 5.

3) Exactly Once

    Can be achieved for kafka to kafka workflows using kafka streams. This also uses idempotent consumer.


Consumer Part 3 Idempotence
---------------------------

Kafka Consumer should always be idempotent. In order to achieve this tweetsId is sent in the request to insert tweets.
This concept of idempotent applies to database inserts as well.

Check out the class - KafkaTweetsConsumerElasticSearchPublisher


Consumer Poll Behavior
----------------------
Kafka Consumers have "Poll" model.
Having poll model helps the consumers on deciding some of the factors like
    - Where in the log they want to consume
    - how fast they want to consume
    - and ability to replay events

            .poll (Duration timeout)
CONSUMER -----------------------------> Broker
         <-----------------------------
         Return data immediately if possible
         OR
         Return "empty" after timeout

Some of the properties are

    - fetch.min.bytes :: default 1 ::
        Controls how much data you want to pull at least in each request.
        Helps improving throughput and decreasing request number
        Costs a little bit of latency

    - max.poll.records :: default 500 ::
        Controls how many records to receive per poll request
        Increase if the messages are very small and lot of RAM is available and you want to consume lot of messages at the same time.
        Good monitor how many records are fetched per poll requests. If necessary increase it to get better throughput

    - max.partition.fetch.bytes :: default 1 MB ::
        Max data returned by the broker per partition
        If you read from lot of partitions like 100 partitions, you will need lots of Memory

    - fetch.max.bytes :: default 50 MB ::
        Maximum data returned for each fetch request (covers multiple partitions)
        Consumer performs multiple fetches in parallel

    - max.poll.interval.ms :: default 300000 - 5 mins ::
        Maximum amount of time between 2 calls of .poll() before declaring a consumer dead
        If processing of data takes time its a good idea to design processing faster.



These are default settings and need not be changed.

Consumer Offset commit Strategies
---------------------------------

2 strategies for committing the offsets

    1) enable.auto.commit = true (default) and synchronous processing of batches.
  This property means there is no new poll on the broker until the current set of records are processed.
    2) enable.auto.commit = false means manually committing the offset when processing finishes.

When enable.auto.commit= true , auto.commit.interval.ms = 5000 by default will be set,
which is 5 secs by default, every time u call .poll()

Its a good idea to have enable.auto.commit = false and do a synchronous processing of batches

Manual commit of Offset
-----------------------
When an unexpected exception happens, then log the exception and continue processing so other messages are consumed and processed.
Consumption Process itself is not put on hold.

Think about a better way of handling these exceptions for practical purposes.


Performance improvements using batching
---------------------------------------
consumeMessageButBulkUpdateElasticSearch() Method is the example of batch updates to Elastic Search.
Since updates happe in batch outside of for loop not able to get the details of offset and partition in the error logging.
Poll duration is mainly determined by the source code - ConsumerRecords<String, String> consumerRecords = kafkaConsumer.poll(Duration.ofMillis(20000));


Consumer Offsets reset Behavior
-------------------------------

If consumer is down for more than the topic's retention policy then offsets are missed/lost
Ex - if a topic's retention policy is 7 days , consumer is down beyond 7 days meaning there is no update on the offset from the consumer
for more than 7 days then, for that consumer offset becomes invalid and the data is lost.

different values for offset reset values are -
auto.offset.reset = latest :: Will read data from the end of the log
auto.offset.reset = earliest :: will read data from the start of the log
auto.offset.reset = none :: Will throw exception if no offset is found

This behavior can be controlled by the broker settings :: offset.retention.minutes


Replaying data
--------------
reset the offset on a specific partition ::
    - kafka-consumer-groups --bootstrap-server localhost:9092 --group tweets-consumer-group --topic twitter-tweets:2 --reset-offsets --to-offset 15 --execute

reset the offset to the start of the log ::
    - kafka-consumer-groups --bootstrap-server localhost:9092 --group tweets-consumer-group --topic twitter-tweets:2 --reset-offsets --to-earliest --execute

Consumer internal threads - Controlling Consumer liveliness
-----------------------------------------------------------

Each consumer polls broker , its called poll thread.
Each Consumer also talks to another Acting Broker called Consumer Coordinator and send heart beats

When any consumer stops sending heart beat, kafka thinks consumer is down and plans for rebalance.

In order for both these functionalities (poll thread and heart beat) to work correctly, it is recommended to
    - process faster and poll often
   otherwise there will be lots of rebalances

For heart beat thread, which happens automatically without our involvement, there is a setting :: session.timeout.ms (default is 10 secs)
If a consumer does not send heart beat in that duration then kafka marks that consumer as dead and rebalances

There is another setting heartbeat.innterval.ms (default 3 secs), this settings specifies the frequency of heart beat consumers send.
General guideline for this setting is 1/3 of session.timeout.ms











Command to try - For grabbing one message from topic
----------------------------------------------------

./bin/kafka-console-consumer.sh --bootstrap-server <broker>:9093 --consumer.config ~/kafka-ssl.prod.properties --topic gom-email --partition 2 --offset 463284684 --max-messages 1



Found a docker-compose file for spinning kafka locally - docker-compose.yml



KOWL - dashboard for kafka topics
docker run -p 8080:8080 -e KAFKA_BROKERS=kafka-ttc-app.dev.target.com:9092 quay.io/cloudhut/kowl:master
Then in the browser type http://localhost:8080/


Use any kafka command with this image but only works on the port 9092 as it cannot read the properties file
    docker run wurstmeister/kafka:2.13-2.8.1 kafka-consumer-groups.sh --bootstrap-server kafka-ttc-app.dev.target.com:9092 --group core_ordersv3_dev_group  --describe

OR to reset the offset or to run the acls (I donâ€™t know this yet)
    docker run wurstmeister/kafka:2.13-2.8.1 kafka-acls.sh --bootstrap-server kafka-ttc-app.dev.target.com:9092 --topic sam-violation-input --list


Issues with librdkafka version -
verify that ssl.ca.location is correctly configured or root CA certificates are installed

cd "$(brew --prefix)"/Library/Taps/homebrew/homebrew-core
git checkout f7d0f40bbc4075177ecf16812fd95951a723a996 -- Formula/librdkafka.rb
cd -
brew unlink librdkafka # If you've already installed librdkafka
brew install librdkafka

OR

You can use librdkafka with librdkafa-2.x.x if you add -X ssl.endpoint.identification.algorithm=none to your kcat





