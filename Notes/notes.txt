Chapter - Kafka Theory
---------------------


Chapter - Starting Kafka
------------------------

brew services start zookeeper

brew services start kafka


To have launchd start zookeeper now and restart at login:
  brew services start zookeeper
Or, if you don't want/need a background service you can just run:
  zkServer start
==> Summary
🍺  /usr/local/Cellar/zookeeper/3.7.0: 1,073 files, 42.4MB
==> Installing kafka
==> Pouring kafka--2.8.0.catalina.bottle.tar.gz
==> Caveats

To have launchd start kafka now and restart at login:
  brew services start kafka
Or, if you don't want/need a background service you can just run:
  zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties & kafka-server-start /usr/local/etc/kafka/server.properties
==> Summary
🍺  /usr/local/Cellar/kafka/2.8.0: 200 files, 68.2MB
==> Caveats

==> zookeeper
To have launchd start zookeeper now and restart at login:
  brew services start zookeeper
Or, if you don't want/need a background service you can just run:
  zkServer start
==> kafka
To have launchd start kafka now and restart at login:
  brew services start kafka
Or, if you don't want/need a background service you can just run:
  zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties & kafka-server-start /usr/local/etc/kafka/server.properties


ZooKeeper
---------
As daemon process -
	zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties
Not as daemon process -
	zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties

Kafka Server
------------
Not as daemon process -
	kafka-server-start /usr/local/etc/kafka/server.properties 
As daemon process -
	kafka-server-start -daemon /usr/local/etc/kafka/server.properties

Start both zookeeper and kafka together
----------------------------------------
zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties & kafka-server-start /usr/local/etc/kafka/server.properties

Another way of starting kafka is - docker container - docker run confluentinc/cp-enterprise-kafka:5.2.2
This didnt work for me as it was asking for zookeeper

Chapter - Command Line Interface 101
------------------------------------

Kafka Topic Commands
-------------------------------
Creating a topic -
	With specific configuration
		kafka-topics --bootstrap-server localhost:9092 --topic first-topic --create --partitions 3 --replication-factor 1
	With all defaults -
		kafka-topics --bootstrap-server localhost:9092 --topic first-topic --create
Listing topics -
	 kafka-topics --bootstrap-server localhost:9092 --list
Description of a topic -
	kafka-topics --bootstrap-server localhost:9092 --topic first-topic --describe
Deleting a topic -
	kafka-topics --bootstrap-server localhost:9092 --topic first-topic --delete

Kafka-Console-Producer Commands
------------------------------------------------
kafka-console-producer --bootstrap-server localhost:9092 --topic first-topic
There will be a little caret , start typing ur messages. If there is no error messages are successfully sent.
Use Ctl+c to exit the console producer

Producing messages to a non-existent topic ::
	kafka-console-producer --bootstrap-server localhost:9092 --topic non-existing-topic
I changed the default # of partitions by editing the server.properties file at /usr/local/etc/kafka and restarted kafka admin client

After that ::
	kafka-console-producer --bootstrap-server localhost:9092 --topic non-existing-topic_2
New non-existing topic non-existing-topic_2 was created with 3 partitions

Teacher's recommendation is to manually create a topic and then produce messages.

Kafka-Console-Consumer Commands
--------------------------------------------------
Basic command for consuming -
	kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic
But with this command Messages sent prior to starting consumer will not be consumed. Only those Messages sent after the consumer started will be consumed.

To read the messages from the beginning of the topic use the command -
	kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic --from-beginning


Kafka-Console-Consumers in a Group
--------------------------------------------------
Command to start the consumers in a group -
	kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic --group second-consumer-group
Command to start the consumers in a new group to read from start of the topic from beginning
	kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic --group second-consumer-group --from-beginning
Command to start a consumer in a group with certs
    kafka-console-consumer --bootstrap-server kafka-tte-app.dev.target.com:9093 --topic cc-serverdb-tap2aws --consumer.config client-ssl.properties --group ccordersv3-local-group --from-beginning


Kafka remembers the offset(index) on every topic that consumer(s) of a group consumed. If u start a consumer of an existing group on existing topic with --from-beginning command it only reads those messages that are not already consumed by an active or a non-active consumer of the same group.
If u start new consumer on new group for a topic that has a bunch of messages on each partition (offset of each partition is > zero) then by default LAG for the new consumer group is zero, meaning new group will not read old messages, will read only new messages, if u want this group to read all the messages from the beginning , then use  --from-beginning option

To see how this works please spin up multiple consumers in the same group


Kafka-consumer-Groups
---------------------------------

List all the consumer groups -
	kafka-consumer-groups --bootstrap-server localhost:9092 --list

Describe a consumer group -
	kafka-consumer-groups --bootstrap-server localhost:9092 --group first-consumer-group --describe
	Lag indicates the delta between CURRENT-OFFSET and LOG-END-OFFSET

Start a consumer of group first-consumer-group in a different Terminal and the run this command to see what output we get
	kafka-consumer-groups --bootstrap-server localhost:9092 --group first-consumer-group --describe
That will list the active consumers and which partitions they are listening to , etc

Reset offset
-----------------
Reset offset will only reset the offset of a specific consumer group
	kafka-consumer-groups --bootstrap-server localhost:9092 --topic first-topic --group first-consumer-group --reset-offsets --to-earliest --execute
Above command resets the offset of first-consumer-group to earliest (0 in this case) , this option is very different from --from-beginning.

	kafka-consumer-groups --bootstrap-server localhost:9092 --group second-consumer-group --topic first-topic --reset-offsets  --shift-by -2  --execute
Above command will reset the offset only by few indexes so the consumer group can read only those and not all from beginning

Resetting offset based on a particular offset
    - kafka-consumer-groups --bootstrap-server kafka-tte-app.prod.target.com:9093 --group ccorders-v3-prod --command-config client-prod-ssl.properties --topic fit-sparta-retail-transactions:0 --reset-offsets  --to-offset 319814955  --execute

Decode and encoded file :: base64 -d -i <input file name> -o <output file name> OR base64 -d copy_and_paste_the_string


Chapter - Kafka Java Programming 101
------------------------------------

Kafka Producer with Callback
----------------------------
public void onCompletion(RecordMetadata metadata, Exception exception) {
// this executes every time a message is successfully sent or an exception is thrown

Kafka Producer with Key
-----------------------
Messages with same key goes the same partition
by default producer.send is asynchronous, it can be synchronous by adding get() method. Look at the BasicProducerWithKey.
Run this sample multiple time to see that same key goes to same partition.

Message Key : id_0
Partition : 1

Message Key : id_1
Partition : 0

Message Key : id_2
Partition : 2

Message Key : id_3
Partition : 0

Message Key : id_4
Partition : 2

Message Key : id_5
Partition : 2

Message Key : id_6
Partition : 0

Message Key : id_7
Partition : 2

Message Key : id_8
Partition : 1

Message Key : id_9
Partition : 2

Kafka Consumer
---------------
For AUTO_OFFSET_RESET_CONFIG , there are 3 values available
    earliest - from the beginning of the topic
    latest - from the point consumer is added to topic onwards
    none - will throw error

One consumer can consume from more than one topic

Java Consumer inside a consumer group
-------------------------------------
you can run the Consumer Java application more than once to create multiple consumers in a group.
When u spin and stop consumers look for messages in the console to check for how many stopped and how re-assignment happens.

Ex - message from BasicConsumer1 after 2 and 3 were started
09:19:30.513 [main] INFO  o.a.k.c.c.i.AbstractCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Attempt to heartbeat failed since group is rebalancing
09:19:30.515 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Revoke previously assigned partitions first-topic-1, first-topic-0
09:19:30.515 [main] INFO  o.a.k.c.c.i.AbstractCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] (Re-)joining group
09:19:30.516 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Finished assignment for group at generation 22: {consumer-first-java-consumer-group-1-6cfecd2a-68ce-47e5-b990-9040dac7edf4=Assignment(partitions=[first-topic-0]), consumer-first-java-consumer-group-1-ffcf31b6-ee27-4e13-96aa-3398669698a0=Assignment(partitions=[first-topic-2]), consumer-first-java-consumer-group-1-948df6b3-b41b-4ede-b7f0-191f95971117=Assignment(partitions=[first-topic-1])}
09:19:30.518 [main] INFO  o.a.k.c.c.i.AbstractCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Successfully joined group with generation 22
09:19:30.518 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Notifying assignor about the new Assignment(partitions=[first-topic-0])
09:19:30.518 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Adding newly assigned partitions: first-topic-0
09:19:30.519 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=consumer-first-java-consumer-group-1, groupId=first-java-consumer-group] Setting offset for partition first-topic-0 to the committed offset FetchPosition{offset=57, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.16:9092 (id: 0 rack: null)], epoch=0}}


Java Consumer with Threads
--------------------------
Refer to ConsumerAsThread.java for example, Its slightly complicated.

Java Consumer Assign and Seek
------------------------------
Assign and Seek are mostly used to replay data and fetch a specific message

Client Bi-Directional Compatibiity
----------------------------------
API calls are now versioned. This means
    older clients (ex - 1.1) can talk to newer brokers (2.0 +)
    newer cluents (2.0 +) can talk to older brokers (1.1)

As far as possible - use latest client irrespective of broker versions

Chapter - Kafka Twitter Producer and Advanced Configurations
-------------------------------------------------------------

kafka-topics --bootstrap-server localhost:9092 --topic twitter-tweets --create --partitions 3 --replication-factor 1

After starting TwitterProducer class , run the console consumer with the command -
    kafka-console-consumer --bootstrap-server localhost:9092 --topic twitter-tweets

        //flush data.
        //Flush actually causes queued records to be sent and blocks until they are completed (successfully or otherwise).
        // Calling get() immediately doesn't trigger anything to happen
        kafkaProducer.flush();

Acks and Min.Insync.replicas
----------------------------

3 levels for acks - 0, 1 and all

acks = 0 - No response is requested from the broker. If broker goes offline or an exception occurs, producer wont know and data will be lost.
This setting is useful when its okay to loose the data. Producer will not know if the message made it to the broker or not.
acks = 0 is good for performance, Good use cases are - metrics, Log collection, etc

acks = 1 - Default settings as of Kafka 2.0 is acks = 1. This meands leader response is requested but replication is not guaranteed. Usually happens
in the backgroud.
If ack is not received then producer may re-try.
Producer knows the broker has the data.
If leader goes down before replication happens the data will be lost.

acks = all - Leader and replicas both are requested to acknowledge. Broker after receives the acknowledgement from the leader and all replicas
sends the ack back to producer. This setting is the safest and adds latency (not much). This is a necessary setting for safe Producer.
If there are enough replica sets with this setting there is no data loss.
With acks = all , there is another setting - min.insync.replicas

min.insync.replicas , is broker or topic setting. broker setting can be overwritten with topic setting.
min.insync.replicas = 2 implies there are atleast 2 brokers that are ISR (including the leader) that must respond with the ack.
min.insync.replicas = 2 is the common setting

If replication factor = 3, min.insync.replicas = 2 and acks = all
    - can only tolerate one broker going down other wise producer will receive an exception on send (NOT_ENOUGH_REPLICAS)

For maximum safety, maximum availability - replication factory = 3, min.insync.replicas 2, and acks = all

Producer retries
----------------

No need to worry about this as my version of kafka is greater than 2.1

Idempotent Producers
--------------------

Kafka >=2.1 has a very high number of retries - 2147483647 by default.
Lets consider a scenario to understand this use case -
1) Producer successfully sent message to Broker, broker received the message and committed but ack sent by broker didnt reach the Producer.
2) Producer since didnt receive the message sent the message again, broker received a duplicate message and sent the ack back
that reached the producer.
3) Per producer there was only 1 successful message but for broker there are duplicates.

Idempotent Producer sends a produce_request_id which is used by the broker to identify duplicate requests, broker commits the message only once

Idempotent Producers are great to guarantee a stable and safe pipeline
by default these are these settings of Producers if using kafka version >= 2.1. These settings apply automatically after producer starts if
u dont overwrite them manually.

retries = Integer.MAX_VALUE (2147483647)
max.in.flight.requests.per.connection = 5 (Kafka >= 1.0) - this is the number of requests per connection. Kafka maintains ordering even if there are retries.
acks = all

Teacher recommends to set these settings manually so its explicit in the code and obvious whats happening.

How do we set idempotent porducers -
    producerProps.put ("enable.idempotence", true)

Safe Producers
---------------

summarizing the settings here -
 - min.insync.replicas = 2 - at the topic or broker level, topic leve settings overwrite the broker level settings

at the Producer level
enable.idempotence =  true
acks = all,
retries = MAX_INT (by default)
max.in.flight.requests.per.connection = 5 (default)

while keeping the ordering guaranteed and improved performance.

PS - Running a safe producer might impact the throughput and latency but its not big

Refer to the TwitterProducer for setting these properties.

Producer Compression
--------------------

Compression is enabled at the Producer level, it does not require any configuration changes either in broker or in consumer.
"compression.type" can be 'none' (default), 'gzip', 'Iz4' and 'snappy'
Compression is more effective when the batch of kafka messages is bigger, more u send to Kafka , compression is more helpful.

Advantages of compression
- much smaller producer request message (compression ratio can go upto 4x)
- Faster to transfer data over the network as message size is smaller
- less latency
- Better throughput
- Better disk Utilization in kafka (Stores messages on disk are smaller)

Disadvantage
- Producer uses some CPU cycles to compression
- Consumers also should use CPU cycles to uncompress the data

gzip has highest compression ratio but its not fast

Producer batching - this is useful in performance improvement
-------------------------------------------------------------

Kafka tries to send the messages as soon as possible to reduce the latency.

max.in.flight.requests.per.connection = 5, upto 5 messages produced individually can be sent at the same time.
If there are more messages ready to be sent while 5 messages are in flight, then kafka starts batching the messages so it can send them all in one go.

Smart batching allows kafka to have high throughput while maintaining low latency.
Batches have higher compression ratio so better efficiency. Each batch is considered one request.

There are 2 settings for batching
- linger.ms and batch,size

linger.ms - Number of millisecs a producer is willing to wait before sending out a batch (default 0)
By introducing a small delay linger.ms = 5, we increase the chances of messages being sent as batch, this batching helps with
higher throughput, efficiency and compression of producer

If batch is full (batch.size) before the linger.ms ends, then producer sends the message to kafka.

batch.size - Maximum number of bytes that will be included in a batch. by default it is 16KB
Any message bigger than the batch.size will not be batched.
batch is allocated per partition

High throughput producer
------------------------


Chapter - Kafka Elasticsearch Consumer and Advanced Configurations
------------------------------------------------------------------

Setting up Elasticsearch in the cloud
-------------------------------------
I setup elasticsearch in the cloud through bonsai.io. user_name is rekha_bg@yahoo.com and password is yokeyyokey
cluster name - KAFKA_STANDALONE_cluster.
URL to access the cluster - https://app.bonsai.io/clusters/kafka-standalone-cou-1278981902/

Elasticsearch 101
-----------------
in the console (click on the Menu item "console" on the left Menu). I executed a REST api to get all the information about the cluster
GET /_cat/nodes?v

to get the list of all indices
GET /_cat/indices?v

Creating an index -
PUT /twitter
Response I got
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "twitter"
}
Then run GET /_cat/indices?v again. Will show details of twitter index

response I got
health status index                     uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   twitter L19J7L2zQUCa91eRIM620g   1   1          0            0       416b           208b

Inserting data into the Index (/twitter) and type (/twitter/tweets) -
PUT /twitter/tweets/1
Body
{
   "course": "Kafka for Beginners",
   "Instructor": "Stephane M",
   "module": "ElasticSearch"
}

Response I received
{
  "_index": "twitter",
  "_type": "tweets",
  "_id": "1",
  "_version": 1,
  "result": "created",
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "_seq_no": 0,
  "_primary_term": 1
}

If u try to PUT again it updates the same record. version in the response will be 2 this time.

if u want to get the data run :: GET /twitter/tweets/1

Response I got
{
  "_index": "twitter",
  "_type": "tweets",
  "_id": "1",
  "_version": 2,
  "_seq_no": 1,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "course": "Kafka for Beginners",
    "Instructor": "Stephane M",
    "module": "ElasticSearch"
  }
}

if u want to delete the record :: DELETE /twitter/tweets/1

If u want to delete the index :: DELETE /twitter
Response
{
  "acknowledged": true
}

Consumer Part - 1
-----------------
Calling the ElasticSearchConsumer class without creating the index "twitter" will fail. Please make sure index "twitter" is available.
U can do that by going to elastic search console and running
GET /twitter
Response I received
{
  "twitter": {
    "aliases": {},
    "mappings": {
      "properties": {
        "Instructor": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "course": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "module": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        }
      }
    },
    "settings": {
      "index": {
        "number_of_shards": "1",
        "auto_expand_replicas": null,
        "provided_name": "twitter",
        "creation_date": "1635781357068",
        "priority": "0",
        "number_of_replicas": "1",
        "uuid": "LozzR9xtQuqhVFE5BTxlWQ",
        "version": {
          "created": "7100299"
        }
      }
    }
  }
}


Consumer Part 2
----------------
Consume the message from twitter topic and send to Elastic search

Delivery Semantics for Consumers
--------------------------------
There are 3 modes of commiting the offsets
1) At Most Once

    Offsets are commited as soon as message batch is received. If processing goes wrong then messages will not be read again.
    there by causing message loss.

    Ex - Lets say 5 messages are read in batch, consumer committed the offset as soon as batch was read so offset is at 5
    processing of 3 went fine and then consumer crashed leaving 2 unprocessed messages.
    In this case When consumer group comes back up it will start reading the messages from offset 5 so 2 previous messages are skipped.

    There is a big chance of loosing data.

2) At Least Once

    Is default behavior. Offsets are committed only after message processing is successful. This might result in duplicate messages.
    Consumer should implement Idempotent processing of data.

    Ex - Lets say 5 messages are read in batch, consumer starts processing the messages.
         Processing of 3 went fine and then consumer crashed leaving 2 unprocessed messages. Offset is not committed yet.
         In this case When consumer group comes back up it will start reading the messages from offset 0 resulting in 3 duplicate messages.
         Once all 5 messages are successfully processed Offset is committed at 5.

3) Exactly Once

    Can be achieved for kafka to kafka workflows using kafka streams. This also uses idempotent consumer.


Consumer Part 3 Idempotence
---------------------------

Kafka Consumer should always be idempotent. In order to achieve this tweetsId is sent in the request to insert tweets.
This concept of idempotent applies to database inserts as well.

Check out the class - KafkaTweetsConsumerElasticSearchPublisher






Command to try - For grabbing one message from topic
----------------------------------------------------

./bin/kafka-console-consumer.sh --bootstrap-server kafka-tte-gom.prod.target.com:9093 --consumer.config ~/kafka-ssl.prod.properties --topic gom-email --partition 2 --offset 463284684 --max-messages 1




